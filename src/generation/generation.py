"""Generation Module for PRODX RAG App

This module generates responses using a HuggingFace LLM, augmented with retrieved context via LangChain.
Supports prompt templating for RAG: Combines query + retrieved docs to guide the LLM.

Key Features:
- Uses HuggingFacePipeline for local LLM inference (e.g., mistral-7b or smaller for demo).
- LangChain chain for RAG: RetrievalQA or custom chain.
- Business Context: Generates solutions for PRODX queries, e.g., "Suggest fix for cryptography dependency conflict" using retrieved logs/docs.

Usage:
  from src.generation.generation import RAGGenerator
  generator = RAGGenerator(llm_model='mistralai/Mistral-7B-Instruct-v0.1')  # Or smaller model
  response = generator.generate("Impact of updating cryptography to 4.0?", retrieved_docs)
  # response: str generated by LLM

Requires: langchain, transformers, torch (for LLM), accelerate.
Note: For production, use AWS SageMaker endpoints or quantized models for efficiency.
"""

from typing import List, Dict, Any, Optional
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_huggingface import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

from src.retrieval.retrieval import Retriever


class RAGGenerator:
    """RAG response generation using LLM with context augmentation."""

    def __init__(self, 
                 llm_model: str = 'distilgpt2',  # Lightweight for demo; use 'mistralai/Mistral-7B-v0.1' for better quality
                 device: str = 'cpu',
                 max_length: int = 512,
                 temperature: float = 0.7,
                 retriever: Optional[Retriever] = None):
        """
        Initialize the generator with LLM pipeline and optional retriever.
        
        Args:
            llm_model: HuggingFace model ID for LLM.
            device: 'cpu' or 'cuda'.
            max_length: Max tokens for generation.
            temperature: Sampling temperature (0.0 deterministic, 1.0 creative).
            retriever: Pre-initialized Retriever for end-to-end RAG.
        """
        self.device = device
        if retriever:
            self.retriever = retriever
        else:
            self.retriever = None  # Can set later
        
        # Load LLM pipeline
        if torch.cuda.is_available() and device == 'cuda':
            model = AutoModelForCausalLM.from_pretrained(llm_model).to(device)
            tokenizer = AutoTokenizer.from_pretrained(llm_model)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            self.llm_pipeline = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=max_length,
                temperature=temperature,
                do_sample=True,
                device=0 if device == 'cuda' else -1
            )
        else:
            self.llm_pipeline = pipeline(
                "text-generation",
                model=llm_model,
                device=-1 if device == 'cpu' else 0,
                max_new_tokens=max_length,
                temperature=temperature,
                do_sample=True
            )
        
        # LangChain wrapper
        self.llm = HuggingFacePipeline(pipeline=self.llm_pipeline)
        
        # Prompt template for RAG (PRODX-specific)
        self.prompt_template = PromptTemplate(
            input_variables=["query", "context"],
            template="""
You are an expert assistant for ABC Bank's PRODX framework. Use the provided context to answer the query about the framework, updates, job failures, or infrastructure impacts. Provide clear, actionable solutions to reduce resolution time for tenants.

Context: {context}

Query: {query}

Response (be concise, explain impacts and fixes):
"""
        )
        
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt_template)

    def format_context(self, docs: List[Dict[str, Any]]) -> str:
        """Format retrieved docs into context string."""
        context = ""
        for doc in docs:
            context += f"Source: {doc['metadata'].get('source', 'Unknown')}\nContent: {doc['content']}\nScore: {doc['score']:.2f}\n\n"
        return context

    def generate(self, query: str, docs: Optional[List[Dict[str, Any]]] = None, k: int = 3) -> str:
        """
        Generate response for query using provided or retrieved docs.
        
        Args:
            query: User query.
            docs: Retrieved documents (if None, use retriever).
            k: Top-k docs if retrieving.
        
        Returns:
            Generated response string.
        """
        if docs is None and self.retriever:
            docs = self.retriever.retrieve(query, k=k)
        elif docs is None:
            raise ValueError("Provide docs or initialize with retriever.")
        
        context = self.format_context(docs)
        response = self.chain.run(query=query, context=context)
        
        # Post-process: Extract only the generated part if needed
        return response.strip()

    def generate_with_retrieval(self, query: str, k: int = 3) -> str:
        """End-to-end: Retrieve and generate."""
        if not self.retriever:
            raise ValueError("Retriever not set. Use generate() with docs.")
        docs = self.retriever.retrieve(query, k=k)
        return self.generate(query, docs)

    def set_retriever(self, retriever: Retriever) -> None:
        """Set retriever for end-to-end usage."""
        self.retriever = retriever

    def get_llm_info(self) -> Dict[str, Any]:
        """Get LLM details."""
        return {
            'model': self.llm.model_id,
            'device': self.device,
            'max_length': self.llm.pipeline.model.generation_config.max_new_tokens
        }


# Example usage (for testing)
if __name__ == "__main__":
    # For demo, use lightweight model
    generator = RAGGenerator(llm_model='distilgpt2')
    # Simulate docs (in practice, from retrieval)
    sample_docs = [
        {'content': 'PRODX update v2.3 causes schema validation failure in Glue.', 'metadata': {'source': 'log.txt'}, 'score': 0.1},
        {'content': 'Fix: Add explicit casting in YAML wrapper.', 'metadata': {'source': 'doc.md'}, 'score': 0.05}
    ]
    response = generator.generate("How to fix Airflow job failure after PRODX update?", sample_docs)
    print(f"Generated response: {response[:200]}...")
    info = generator.get_llm_info()
    print(f"LLM: {info['model']} on {info['device']}")